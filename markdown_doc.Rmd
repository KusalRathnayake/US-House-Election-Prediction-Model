---
title: "Machine Leaning Models for the Prediction of Us House Election Outcome Using District Level Social and Economic Predictors"
output:
  pdf_document: default
  html_notebook: default
---

In this study, 10 district level demographic and socio-economic variables were used to build a prediction model of US house election outcome. 

Predictor Variables:

1. White %

2. Black %

3. Asian %

4. Hispanic %

5. Female %

6. Age 18 to 55 %

7. Age 55 plus %

8. International immigration count

9. Farming personnel %

10. Personal income

Dependent Variable: Democratic win (1: win or 0: lose)

Logistic regression and point biserial correlation were used to identify the significance of the predictor variables.
Support vector machines, Decision trees, Random Forests and ANN methods were used to build the prediction model and the model classification accuracies were compared.


```{r}
library(e1071)
library(rpart)
library(randomForest)
library(tidyverse)
library(ggplot2)
library(sf)
library(ggmap)
```

##### Attach datasets from 2000 to 2020 into dataframes and row-bind the datasets into one dataframe

```{r}
df00 <- as.data.frame(df_2000)
df02 <- as.data.frame(df_2002)
df04 <- as.data.frame(df_2004)
df06 <- as.data.frame(df_2006)
df08 <- as.data.frame(df_2008)
df10 <- as.data.frame(df_2010)
df12 <- as.data.frame(df_2012)
df14 <- as.data.frame(df_2014)
df16 <- as.data.frame(df_2016)
df18 <- as.data.frame(df_2018)
df20 <- as.data.frame(df_2020)

tr_df <- rbind(df00,df02,df04,df06,df08,df10,df12,df14,df16)
```

##### Create dependent variables as in the description

```{r}
tr_df$yvar <- NA

for (i in 1:length(tr_df$win_party)) 
{
  if(tr_df$win_party[i] == 1 |tr_df$win_party[i] == 3 )
    tr_df$yvar[i] = 1
  else
    tr_df$yvar[i] = 0
}

df18$yvar <- NA

for (i in 1:length(df18$win_party)) 
{
  if(df18$win_party[i] == 1 |df18$win_party[i] == 3 )
    df18$yvar[i] = 1
  else
    df18$yvar[i] = 0
}

```

##### Create training and test set
training set includes data from 2000 to 2016

test set includes 2018 data

```{r}
training_set <- data.frame('year' = tr_df$year, 'state' = tr_df$state, 'district' = tr_df$district, 'win_party' = tr_df$win_party,  'white' = (tr_df$white*100),'black'=(tr_df$black*100),'asian'=(tr_df$asian*100),'hispanic'=(tr_df$hispanic*100),'male' = (tr_df$male*100), 'female'=(tr_df$female*100),'age18_55'=(tr_df$age_18_55*100),'age_55p'=(tr_df$age_55_plus),'interM'=scale(tr_df$interM),'domesM'=scale(tr_df$domesM), 'farm'=(tr_df$farm_E_I*100),'nonfarm'=(tr_df$nonfarm_E*100), 'personal'=log10(tr_df$personal),'yvar'=factor(tr_df$yvar))


test_set <- data.frame('year' = df18$year, 'state' = df18$state, 'district' = df18$district, 'win_party' = df18$win_party, 'white' =(df18$white*100),'black'=(df18$black*100),'asian'=(df18$asian*100), 'hispanic'=(df18$hispanic*100),'male' = (df18$male*100), 'female'=(df18$female*100), 'age18_55'=(df18$age_18_55*100), 'age_55p'=(df18$age_55_plus),'interM'=scale(df18$interM),           'domesM'=scale(df18$domesM), 'farm'=(df18$farm_E_I*100),'nonfarm'=(df18$nonfarm_E*100), 'personal'=log10(df18$personal),        'yvar'=factor(df18$yvar))

```

##### Point Biserial Correlations

```{r}
cor.test(tr_df$white,tr_df$yvar)
cor.test(tr_df$black,tr_df$yvar)
cor.test(tr_df$asian,tr_df$yvar)
cor.test(tr_df$hispanic,tr_df$yvar)
cor.test(tr_df$female,tr_df$yvar)
cor.test(tr_df$age_18_55,tr_df$yvar)
cor.test(tr_df$age_55_plus,tr_df$yvar)
cor.test(tr_df$interM,tr_df$yvar)
cor.test(tr_df$domesM,tr_df$yvar)
cor.test(tr_df$farm_E_I,tr_df$yvar)
cor.test(tr_df$nonfarm_E,tr_df$yvar)
cor.test(tr_df$personal,tr_df$yvar)
```

##### Support Vector Machines (SVM) Clasification Model

```{r}
# SVM Classification

clasifier <- svm(factor(yvar)~white+black+asian+hispanic+male+female+age18_55+age_55p+interM+domesM+farm+nonfarm+personal,data = training_set, type = 'C-classification')

summary(classifier)

ypred <- predict(classifier,newdata = test_set[-18])

yact <- test_set$yvar
cm <- table(yact, ypred)
cm
```

##### Decision trees Clasification Model

```{r}
# Decision Tree Classification

training_set <- na.omit(training_set)

classifier <- rpart(factor(yvar)~white+black+asian+hispanic+female+age18_55+age_55p+interM+domesM+farm+personal, data = training_set)
ypred <- predict(classifier,newdata = test_set[-18],type = 'class')

yact <- test_set$yvar
cm <- table(yact, ypred)
cm
```

##### Random Forests (RF) Clasification Model

```{r}
# Random Forest Classification

classifier <- randomForest(x = training_set[-c(4,18)],y = training_set$yvar,ntree = 300)
ypred <- predict(classifier,newdata = test_set[-c(4,18)])

yact <- test_set$yvar
cm <- table(yact, ypred)
cm

```

According to the classification accuracy, Random forests model turned out to be the superior model out of the 3 machine learning methods.

Then the Random forests model was compared with the classification accuracy of an Artificial neural network classification model.

##### Artificial Neural Network (ANN) Clasification Model

```{r}
# ANN Analysis

library(h2o)

tr_ANN <- data.frame('year' = tr_df$year, 'state' = tr_df$state, 'district' = tr_df$district, 'win_party' = tr_df$win_party, 'white' = (tr_df$white*100),'black'=(tr_df$black*100),'asian'=(tr_df$asian*100),'hispanic'=(tr_df$hispanic*100),'male' = (tr_df$male*100), 'female'=(tr_df$female*100),'age18_55'=(tr_df$age_18_55*100), 'age_55p'=(tr_df$age_55_plus),'interM'=tr_df$interM, 'domesM'=tr_df$domesM,'farm'=(tr_df$farm_E_I*100),'nonfarm'=(tr_df$nonfarm_E*100), 'personal'=tr_df$personal, 'yvar' = factor(tr_df$yvar))

training_ANN <- tr_ANN[,5:18]
training_ANN <- na.omit(training_ANN)

ts_ANN <- data.frame('year' = df18$year, 'state' = df18$state, 'district' = df18$district, 'win_party' = df18$win_party, 'white' = (df18$white*100),'black'=(df18$black*100),'asian'=(df18$asian*100),'hispanic'=(df18$hispanic*100),'male' = (df18$male*100), 'female'=(df18$female*100),'age18_55'=(df18$age_18_55*100), 'age_55p'=(df18$age_55_plus),'interM'=df18$interM, 'domesM'=df18$domesM, 'farm'=(df18$farm_E_I*100),'nonfarm'=(df18$nonfarm_E*100), 'personal'=df18$personal, 'yvar' = factor(df18$yvar))

test_ANN <- ts_ANN[,5:18]

h2o.init(nthreads = -1)
classifier_ANN <- h2o.deeplearning(y = 'yvar',
                               training_frame = as.h2o(training_ANN),
                               activation = 'Rectifier',
                               hidden = c(15,15,15),
                               epochs = 200,
                               train_samples_per_iteration = -2)

prob_pred_ANN <- h2o.predict(classifier_ANN,newdata = as.h2o(test_ANN[-14]))

prob_pred_ANN <- as.vector(prob_pred_ANN)
ypred_ANN <- as.factor(prob_pred_ANN[1:435])

yact <- test_ANN$yvar
cm <- table(yact, ypred_ANN)
cm

h2o.shutdown()

```

The results indicated that the classification accuracy of RF model is superior than the classification accuracy of ANN model.

The RF model was then used for the prediction of 2020 election outcome, and the results indicated a classification accuracy rate of 95% for 2020 US house election outcome.

















